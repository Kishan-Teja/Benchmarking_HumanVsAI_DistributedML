{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "INPUT_DIM = 768\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 33\n",
    "NUM_LAYERS = 2\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "DROPOUT = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.embeddings[idx], dtype=torch.float32), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, dropout=DROPOUT, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.lstm(x)\n",
    "        return self.fc(output[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    train_data = pd.read_hdf('train_embeddings.h5')\n",
    "    test_data = pd.read_hdf('test_embeddings.h5')\n",
    "    val_data = pd.read_hdf('val_embeddings.h5')\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_data['source'] = label_encoder.fit_transform(train_data['source'])\n",
    "    test_data['source'] = label_encoder.transform(test_data['source'])\n",
    "    val_data['source'] = label_encoder.transform(val_data['source'])\n",
    "\n",
    "    train_dataset = EmbeddingDataset(np.array(train_data['gpt2_embeddings']), np.array(train_data['source']))\n",
    "    test_dataset = EmbeddingDataset(np.array(test_data['gpt2_embeddings']), np.array(test_data['source']))\n",
    "    val_dataset = EmbeddingDataset(np.array(val_data['gpt2_embeddings']), np.array(val_data['source']))\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    test_sampler = DistributedSampler(test_dataset, num_replicas=world_size, rank=rank)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "\n",
    "    model = LSTMModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "\n",
    "    optimizer = optim.Adam(ddp_model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        ddp_model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(rank), target.to(rank)\n",
    "            optimizer.zero_grad()\n",
    "            output = ddp_model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate time and resource usage\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        gpu_memory = torch.cuda.memory_allocated(rank) / (1024 ** 3)\n",
    "        system_memory = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        ddp_model.eval()\n",
    "        val_preds, val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(rank), target.to(rank)\n",
    "                output = ddp_model(data)\n",
    "                val_preds.extend(output.argmax(dim=1).cpu().numpy())\n",
    "                val_labels.extend(target.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(val_labels, val_preds)\n",
    "        precision = precision_score(val_labels, val_preds, average='macro')\n",
    "        recall = recall_score(val_labels, val_preds, average='macro')\n",
    "        f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f'Epoch {epoch+1}: Loss={total_loss/len(train_loader)}, Time={epoch_time}, GPU Mem={gpu_memory}GB, Sys Mem={system_memory}GB')\n",
    "            print(f'Val Metrics - Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')\n",
    "\n",
    "    cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "torch.multiprocessing.spawn(train, args=(world_size,), nprocs=world_size, join=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
